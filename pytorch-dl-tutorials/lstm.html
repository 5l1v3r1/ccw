<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>LSTM Networks for Sentiment Analysis &#8212; DeepLearning 0.1 documentation</title>
    
    <link rel="stylesheet" href="_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Modeling and generating sequences of polyphonic music with the RNN-RBM" href="rnnrbm.html" />
    <link rel="prev" title="Recurrent Neural Networks with Word Embeddings" href="rnnslu.html" /> 
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="rnnrbm.html" title="Modeling and generating sequences of polyphonic music with the RNN-RBM"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="rnnslu.html" title="Recurrent Neural Networks with Word Embeddings"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="contents.html">DeepLearning 0.1 documentation</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="contents.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">LSTM Networks for Sentiment Analysis</a><ul>
<li><a class="reference internal" href="#summary">Summary</a></li>
<li><a class="reference internal" href="#data">Data</a></li>
<li><a class="reference internal" href="#model">Model</a><ul>
<li><a class="reference internal" href="#id1">LSTM</a></li>
<li><a class="reference internal" href="#our-model">Our model</a></li>
</ul>
</li>
<li><a class="reference internal" href="#code-citations-contact">Code - Citations - Contact</a><ul>
<li><a class="reference internal" href="#code">Code</a></li>
<li><a class="reference internal" href="#papers">Papers</a></li>
<li><a class="reference internal" href="#contact">Contact</a></li>
</ul>
</li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="rnnslu.html"
                        title="previous chapter">Recurrent Neural Networks with Word Embeddings</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="rnnrbm.html"
                        title="next chapter">Modeling and generating sequences of polyphonic music with the RNN-RBM</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/lstm.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="lstm-networks-for-sentiment-analysis">
<span id="lstm"></span><h1>LSTM Networks for Sentiment Analysis<a class="headerlink" href="#lstm-networks-for-sentiment-analysis" title="Permalink to this headline">¶</a></h1>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>This tutorial aims to provide an example of how a Recurrent Neural Network
(RNN) using the Long Short Term Memory (LSTM) architecture can be implemented
using Theano. In this tutorial, this model is used to perform sentiment
analysis on movie reviews from the <a class="reference external" href="http://ai.stanford.edu/~amaas/data/sentiment/">Large Movie Review Dataset</a>, sometimes known as the
IMDB dataset.</p>
<p>In this task, given a movie review, the model attempts to predict whether it
is positive or negative. This is a binary classification task.</p>
</div>
<div class="section" id="data">
<h2>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h2>
<p>As previously mentioned, the provided scripts are used to train a LSTM
recurrent neural network on the Large Movie Review Dataset dataset.</p>
<p>While the dataset is public, in this tutorial we provide a copy of the dataset
that has previously been preprocessed according to the needs of this LSTM
implementation. Running the code provided in this tutorial will automatically
download the data to the local directory. In order to use your own data, please
use a (<a class="reference external" href="https://raw.githubusercontent.com/kyunghyuncho/DeepLearningTutorials/master/code/imdb_preprocess.py">preprocessing script</a>)
provided as a part of this tutorial.</p>
<p>Once the model is trained, you can test it with your own corpus using the
word-index dictionary
(<a class="reference external" href="http://www.iro.umontreal.ca/~lisa/deep/data/imdb.dict.pkl.gz">imdb.dict.pkl.gz</a>)
provided as a part of this tutorial.</p>
</div>
<div class="section" id="model">
<h2>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3>LSTM<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>In a <em>traditional</em> recurrent neural network, during the gradient
back-propagation phase, the gradient signal can end up being multiplied a
large number of times (as many as the number of timesteps) by the weight
matrix associated with the connections between the neurons of the recurrent
hidden layer. This means that, the magnitude of weights in the transition
matrix can have a strong impact on the learning process.</p>
<p>If the weights in this matrix are small (or, more formally, if the leading
eigenvalue of the weight matrix is smaller than 1.0), it can lead to a
situation called <em>vanishing gradients</em> where the gradient signal gets so small
that learning either becomes very slow or stops working altogether. It can
also make more difficult the task of learning long-term dependencies in the
data. Conversely, if the weights in this matrix are large (or, again, more
formally, if the leading eigenvalue of the weight matrix is larger than 1.0),
it can lead to a situation where the gradient signal is so large that it can
cause learning to diverge. This is often referred to as <em>exploding gradients</em>.</p>
<p>These issues are the main motivation behind the LSTM model which introduces a
new structure called a <em>memory cell</em> (see Figure 1 below). A memory cell is
composed of four main elements: an input gate, a neuron with a self-recurrent
connection (a connection to itself), a forget gate and an output gate. The
self-recurrent connection has a weight of 1.0 and ensures that, barring any
outside interference, the state of a memory cell can remain constant from one
timestep to another. The gates serve to modulate the interactions between the
memory cell itself and its environment. The input gate can allow incoming
signal to alter the state of the memory cell or block it. On the other hand,
the output gate can allow the state of the memory cell to have an effect on
other neurons or prevent it. Finally, the forget gate can modulate the memory
cell’s self-recurrent connection, allowing the cell to remember or forget its
previous state, as needed.</p>
<div class="figure align-center" id="id2">
<img alt="_images/lstm_memorycell.png" src="_images/lstm_memorycell.png" />
<p class="caption"><span class="caption-text"><strong>Figure 1</strong>: Illustration of an LSTM memory cell.</span></p>
</div>
<p>The equations below describe how a layer of memory cells is updated at every
timestep <img class="math" src="_images/math/4129d14426df0cf11bade24233b1eeb71fc842a3.png" alt="t"/>. In these equations:</p>
<ul class="simple">
<li><img class="math" src="_images/math/7751dc8de9af56139694ee1f470046de9c672fa2.png" alt="x_t"/> is the input to the memory cell layer at time <img class="math" src="_images/math/4129d14426df0cf11bade24233b1eeb71fc842a3.png" alt="t"/></li>
<li><img class="math" src="_images/math/277cc599a7ac8d41807adf26534a24fb5fd2fb7e.png" alt="W_i"/>, <img class="math" src="_images/math/02a202fcece1161dac1862cd13285c45aa0a09f7.png" alt="W_f"/>, <img class="math" src="_images/math/d16b45fa4438f0ed882f29f4ceac2bab410ecfd5.png" alt="W_c"/>, <img class="math" src="_images/math/f71411eef652f65304cd55ceb52a3a754c5c9d82.png" alt="W_o"/>, <img class="math" src="_images/math/7a754f49f8c819682f71d36f2ba83db86a5f20ca.png" alt="U_i"/>,
<img class="math" src="_images/math/f91851269b76b5a8348a618ce316f29fce197b50.png" alt="U_f"/>, <img class="math" src="_images/math/a512441f441806e1e0648fc16ccb8189a320e8c8.png" alt="U_c"/>, <img class="math" src="_images/math/627268a4b1d32ae9e77d96ae33caa777f30cbad4.png" alt="U_o"/> and <img class="math" src="_images/math/a92be257b314bf420459c3f34bd7b02c9b526a69.png" alt="V_o"/> are weight
matrices</li>
<li><img class="math" src="_images/math/c986b6f2ebcc19c8b793937383160090ab83a774.png" alt="b_i"/>, <img class="math" src="_images/math/47d9687366832e72778ae8d8a6700b2ed62cb81a.png" alt="b_f"/>, <img class="math" src="_images/math/a21817ef4f2c314717b04129965ad1a03d90636d.png" alt="b_c"/> and <img class="math" src="_images/math/94c710d128195a0ee44bf13c500c48b4362962cf.png" alt="b_o"/> are bias vectors</li>
</ul>
<p>First, we compute the values for <img class="math" src="_images/math/61fbadda1edf0e683164d61523d233bca6253364.png" alt="i_t"/>, the input gate, and
<img class="math" src="_images/math/b1bee1f111c3b75006c4e028c712e3bfb41c0896.png" alt="\widetilde{C_t}"/> the candidate value for the states of the memory
cells at time <img class="math" src="_images/math/4129d14426df0cf11bade24233b1eeb71fc842a3.png" alt="t"/>:</p>
<div class="math" id="equation-1">
<p><span class="eqno">(1)</span><img src="_images/math/8d15deb3b21279002456efccc157f93c2a759f07.png" alt="i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i)"/></p>
</div><div class="math" id="equation-2">
<p><span class="eqno">(2)</span><img src="_images/math/d9894c56ddac28b00fba2aeb21d2019747bf1a5b.png" alt="\widetilde{C_t} = tanh(W_c x_t + U_c h_{t-1} + b_c)"/></p>
</div><p>Second, we compute the value for <img class="math" src="_images/math/1865cba104ad6006473e955e0da18da919696af7.png" alt="f_t"/>, the activation of the memory
cells&#8217; forget gates at time <img class="math" src="_images/math/4129d14426df0cf11bade24233b1eeb71fc842a3.png" alt="t"/>:</p>
<div class="math" id="equation-3">
<p><span class="eqno">(3)</span><img src="_images/math/74d600585435eeacd3e146d8fe4c7aaad0e78d02.png" alt="f_t = \sigma(W_f x_t + U_f h_{t-1} + b_f)"/></p>
</div><p>Given the value of the input gate activation <img class="math" src="_images/math/61fbadda1edf0e683164d61523d233bca6253364.png" alt="i_t"/>, the forget gate
activation <img class="math" src="_images/math/1865cba104ad6006473e955e0da18da919696af7.png" alt="f_t"/> and the candidate state value <img class="math" src="_images/math/b1bee1f111c3b75006c4e028c712e3bfb41c0896.png" alt="\widetilde{C_t}"/>,
we can compute <img class="math" src="_images/math/42fadd196a2a5250d31657ff2ab5eb6078fde28a.png" alt="C_t"/> the memory cells&#8217; new state at time <img class="math" src="_images/math/4129d14426df0cf11bade24233b1eeb71fc842a3.png" alt="t"/>:</p>
<div class="math" id="equation-4">
<p><span class="eqno">(4)</span><img src="_images/math/6252b09e2bba34f2aa95c254caf4bcb969d9019f.png" alt="C_t = i_t * \widetilde{C_t} + f_t * C_{t-1}"/></p>
</div><p>With the new state of the memory cells, we can compute the value of their
output gates and, subsequently, their outputs:</p>
<div class="math" id="equation-5">
<p><span class="eqno">(5)</span><img src="_images/math/7af866f7ad61180b2f6a845eeba928a3de68ff6b.png" alt="o_t = \sigma(W_o x_t + U_o h_{t-1} + V_o C_t + b_o)"/></p>
</div><div class="math" id="equation-6">
<p><span class="eqno">(6)</span><img src="_images/math/91c8423c9c4f5c8af9312a5883c7c4a9efa6bc99.png" alt="h_t = o_t * tanh(C_t)"/></p>
</div></div>
<div class="section" id="our-model">
<h3>Our model<a class="headerlink" href="#our-model" title="Permalink to this headline">¶</a></h3>
<p>The model we used in this tutorial is a variation of the standard LSTM model.
In this variant, the activation of a cell’s output gate does not depend on the
memory cell’s state <img class="math" src="_images/math/42fadd196a2a5250d31657ff2ab5eb6078fde28a.png" alt="C_t"/>. This allows us to perform part of the
computation more efficiently (see the implementation note, below, for
details). This means that, in the variant we have implemented, there is no
matrix <img class="math" src="_images/math/a92be257b314bf420459c3f34bd7b02c9b526a69.png" alt="V_o"/> and equation <a class="reference internal" href="#equation-5">(5)</a> is replaced by equation <a class="reference internal" href="#equation-5-alt">(7)</a>:</p>
<div class="math" id="equation-5-alt">
<p><span class="eqno">(7)</span><img src="_images/math/5c0b58d6766996ee49a7bc84a8252448c9a8824a.png" alt="o_t = \sigma(W_o x_t + U_o h_{t-1} + b_o)"/></p>
</div><p>Our model is composed of a single LSTM layer followed by an average pooling
and a logistic regression layer as illustrated in Figure 2 below. Thus, from
an input sequence <img class="math" src="_images/math/d142afcef3f1eb76eb11148e76561ba3c8dd0b5b.png" alt="x_0, x_1, x_2, ..., x_n"/>, the memory cells in the
LSTM layer will produce a representation sequence <img class="math" src="_images/math/80ae3b89551a6cf191cbe1aa17c6c556e0d1eaaa.png" alt="h_0, h_1, h_2, ...,
h_n"/>. This representation sequence is then averaged over all timesteps
resulting in representation h. Finally, this representation is fed to a
logistic regression layer whose target is the class label associated with the
input sequence.</p>
<div class="figure align-center" id="id3">
<img alt="_images/lstm.png" src="_images/lstm.png" />
<p class="caption"><span class="caption-text"><strong>Figure 2</strong> : Illustration of the model used in this tutorial. It is
composed of a single LSTM layer followed by mean pooling over time and
logistic regression.</span></p>
</div>
<p><strong>Implementation note</strong> : In the code included this tutorial, the equations
<a class="reference internal" href="#equation-1">(1)</a>, <a class="reference internal" href="#equation-2">(2)</a>, <a class="reference internal" href="#equation-3">(3)</a> and <a class="reference internal" href="#equation-5-alt">(7)</a> are performed in parallel to make
the computation more efficient. This is possible because none of these
equations rely on a result produced by the other ones. It is achieved by
concatenating the four matrices <img class="math" src="_images/math/c4c67d6ce5fe42e3298fe1ba916e5f8171d5d0d7.png" alt="W_*"/> into a single weight matrix
<img class="math" src="_images/math/597c3a32b95a572264f23d8ac380dea3e50e0cc0.png" alt="W"/> and performing the same concatenation on the weight matrices
<img class="math" src="_images/math/f23c55a4692749288378777a978582b859d985c4.png" alt="U_*"/> to produce the matrix <img class="math" src="_images/math/10fafa8aa777f2b1e8dbaa2c5cfa0299c61fe961.png" alt="U"/> and the bias vectors <img class="math" src="_images/math/fbda58a8e14e7fc258e717c1e3eaef87f1de027a.png" alt="b_*"/>
to produce the vector <img class="math" src="_images/math/30f56f8625112194b5cbc79b78f8213604a50dd2.png" alt="b"/>. Then, the pre-nonlinearity activations can
be computed with:</p>
<div class="math">
<p><img src="_images/math/a8f61b34943f54937b0d16ba6ef8f732996ec768.png" alt="z = W x_t + U h_{t-1} + b"/></p>
</div><p>The result is then sliced to obtain the pre-nonlinearity activations for
<img class="math" src="_images/math/21d98334101b86128698b3b3e441168f62e89905.png" alt="i"/>, <img class="math" src="_images/math/f3182db99b3f6b5f9e9c056d47b66f1e8bb4206f.png" alt="f"/>, <img class="math" src="_images/math/b1bee1f111c3b75006c4e028c712e3bfb41c0896.png" alt="\widetilde{C_t}"/>, and <img class="math" src="_images/math/13359f35449287284eb052f3770db52eb08e4317.png" alt="o"/> and the
non-linearities are then applied independently for each.</p>
</div>
</div>
<div class="section" id="code-citations-contact">
<h2>Code - Citations - Contact<a class="headerlink" href="#code-citations-contact" title="Permalink to this headline">¶</a></h2>
<div class="section" id="code">
<h3>Code<a class="headerlink" href="#code" title="Permalink to this headline">¶</a></h3>
<p>The LSTM implementation can be found in the two following files:</p>
<ul class="simple">
<li><a class="reference external" href="http://deeplearning.net/tutorial/code/lstm.py">lstm.py</a>: Main script. Defines and train the model.</li>
<li><a class="reference external" href="http://deeplearning.net/tutorial/code/imdb.py">imdb.py</a>: Secondary script. Handles the loading and preprocessing of the IMDB dataset.</li>
</ul>
<p>After downloading both scripts and putting both in the same folder, the user
can run the code by calling:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span><span class="nv">THEANO_FLAGS</span><span class="o">=</span><span class="s2">&quot;floatX=float32&quot;</span> python lstm.py
</pre></div>
</div>
<p>The script will automatically download the data and decompress it.</p>
<p><strong>Note</strong>: The provided code supports the Stochastic Gradient Descent (SGD),
AdaDelta and RMSProp optimization methods. You are advised to use AdaDelta or
RMSProp because SGD appears to performs poorly on this task with this
particular model.</p>
</div>
<div class="section" id="papers">
<h3>Papers<a class="headerlink" href="#papers" title="Permalink to this headline">¶</a></h3>
<p>If you use this tutorial, please cite the following papers.</p>
<p>Introduction of the LSTM model:</p>
<ul class="simple">
<li><a class="reference external" href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf">[pdf]</a> Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.</li>
</ul>
<p>Addition of the forget gate to the LSTM model:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015">[pdf]</a> Gers, F. A., Schmidhuber, J., &amp; Cummins, F. (2000). Learning to forget: Continual prediction with LSTM. Neural computation, 12(10), 2451-2471.</li>
</ul>
<p>More recent LSTM paper:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.cs.toronto.edu/~graves/preprint.pdf">[pdf]</a> Graves, Alex. Supervised sequence labelling with recurrent neural networks. Vol. 385. Springer, 2012.</li>
</ul>
<p>Papers related to Theano:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.iro.umontreal.ca/~lisa/pointeurs/nips2012_deep_workshop_theano_final.pdf">[pdf]</a> Bastien, Frédéric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian, Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements. NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2012.</li>
<li><a class="reference external" href="http://www.iro.umontreal.ca/~lisa/pointeurs/theano_scipy2010.pdf">[pdf]</a> Bergstra, James, Breuleux, Olivier, Bastien, Frédéric, Lamblin, Pascal, Pascanu, Razvan, Desjardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010.</li>
</ul>
<p>Thank you!</p>
</div>
<div class="section" id="contact">
<h3>Contact<a class="headerlink" href="#contact" title="Permalink to this headline">¶</a></h3>
<p>Please email <a class="reference external" href="mailto:carriepl&#46;&#46;at&#46;&#46;iro&#46;umontreal&#46;ca">Pierre Luc Carrier</a> or
<a class="reference external" href="http://www.kyunghyuncho.me/">Kyunghyun Cho</a> for any problem report or
feedback. We will be glad to hear from you.</p>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Hochreiter, S., &amp; Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8), 1735-1780.</li>
<li>Gers, F. A., Schmidhuber, J., &amp; Cummins, F. (2000). Learning to forget: Continual prediction with LSTM. Neural computation, 12(10), 2451-2471.</li>
<li>Graves, A. (2012). Supervised sequence labelling with recurrent neural networks (Vol. 385). Springer.</li>
<li>Hochreiter, S., Bengio, Y., Frasconi, P., &amp; Schmidhuber, J. (2001). Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.</li>
<li>Bengio, Y., Simard, P., &amp; Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. Neural Networks, IEEE Transactions on, 5(2), 157-166.</li>
<li>Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., &amp; Potts, C. (2011, June). Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1 (pp. 142-150). Association for Computational Linguistics.</li>
</ul>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="rnnrbm.html" title="Modeling and generating sequences of polyphonic music with the RNN-RBM"
             >next</a> |</li>
        <li class="right" >
          <a href="rnnslu.html" title="Recurrent Neural Networks with Word Embeddings"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="contents.html">DeepLearning 0.1 documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2008--2010, LISA lab.
      Last updated on Sep 25, 2017.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.5.1.
    </div>
  </body>
</html>