<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Recurrent Neural Networks with Word Embeddings &#8212; DeepLearning 0.1 documentation</title>
    
    <link rel="stylesheet" href="_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="LSTM Networks for Sentiment Analysis" href="lstm.html" />
    <link rel="prev" title="Hybrid Monte-Carlo Sampling" href="hmc.html" /> 
  </head>
  <body role="document">
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="lstm.html" title="LSTM Networks for Sentiment Analysis"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="hmc.html" title="Hybrid Monte-Carlo Sampling"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="contents.html">DeepLearning 0.1 documentation</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="contents.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Recurrent Neural Networks with Word Embeddings</a><ul>
<li><a class="reference internal" href="#summary">Summary</a></li>
<li><a class="reference internal" href="#code-citations-contact">Code - Citations - Contact</a><ul>
<li><a class="reference internal" href="#code">Code</a></li>
<li><a class="reference internal" href="#papers">Papers</a></li>
<li><a class="reference internal" href="#contact">Contact</a></li>
</ul>
</li>
<li><a class="reference internal" href="#task">Task</a></li>
<li><a class="reference internal" href="#dataset">Dataset</a></li>
<li><a class="reference internal" href="#recurrent-neural-network-model">Recurrent Neural Network Model</a><ul>
<li><a class="reference internal" href="#raw-input-encoding">Raw input encoding</a></li>
<li><a class="reference internal" href="#context-window">Context window</a></li>
<li><a class="reference internal" href="#word-embeddings">Word embeddings</a></li>
<li><a class="reference internal" href="#elman-recurrent-neural-network">Elman recurrent neural network</a></li>
</ul>
</li>
<li><a class="reference internal" href="#evaluation">Evaluation</a></li>
<li><a class="reference internal" href="#training">Training</a><ul>
<li><a class="reference internal" href="#updates">Updates</a></li>
<li><a class="reference internal" href="#stopping-criterion">Stopping Criterion</a></li>
<li><a class="reference internal" href="#hyper-parameter-selection">Hyper-Parameter Selection</a></li>
</ul>
</li>
<li><a class="reference internal" href="#running-the-code">Running the Code</a><ul>
<li><a class="reference internal" href="#timing">Timing</a></li>
<li><a class="reference internal" href="#word-embedding-nearest-neighbors">Word Embedding Nearest Neighbors</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="hmc.html"
                        title="previous chapter">Hybrid Monte-Carlo Sampling</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="lstm.html"
                        title="next chapter">LSTM Networks for Sentiment Analysis</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/rnnslu.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="recurrent-neural-networks-with-word-embeddings">
<span id="rnnslu"></span><h1>Recurrent Neural Networks with Word Embeddings<a class="headerlink" href="#recurrent-neural-networks-with-word-embeddings" title="Permalink to this headline">¶</a></h1>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>In this tutorial, you will learn how to:</p>
<ul class="simple">
<li>learn <strong>Word Embeddings</strong></li>
<li>using <strong>Recurrent Neural Networks</strong> architectures</li>
<li>with <strong>Context Windows</strong></li>
</ul>
<p>in order to perform Semantic Parsing / Slot-Filling (Spoken Language Understanding)</p>
</div>
<div class="section" id="code-citations-contact">
<h2>Code - Citations - Contact<a class="headerlink" href="#code-citations-contact" title="Permalink to this headline">¶</a></h2>
<div class="section" id="code">
<h3>Code<a class="headerlink" href="#code" title="Permalink to this headline">¶</a></h3>
<p>Directly running experiments is also possible using this <a class="reference external" href="https://github.com/mesnilgr/is13">github repository</a>.</p>
</div>
<div class="section" id="papers">
<h3>Papers<a class="headerlink" href="#papers" title="Permalink to this headline">¶</a></h3>
<p>If you use this tutorial, cite the following papers:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.iro.umontreal.ca/~lisa/pointeurs/RNNSpokenLanguage2013.pdf">[pdf]</a> Grégoire Mesnil, Xiaodong He, Li Deng and Yoshua Bengio. Investigation of Recurrent-Neural-Network Architectures and Learning Methods for Spoken Language Understanding. Interspeech, 2013.</li>
<li><a class="reference external" href="http://research.microsoft.com/en-us/people/gokhant/0000019.pdf">[pdf]</a> Gokhan Tur, Dilek Hakkani-Tur and Larry Heck. What is left to be understood in ATIS?</li>
<li><a class="reference external" href="http://lia.univ-avignon.fr/fileadmin/documents/Users/Intranet/fich_art/997-Interspeech2007.pdf">[pdf]</a> Christian Raymond and Giuseppe Riccardi. Generative and discriminative algorithms for spoken language understanding. Interspeech, 2007.</li>
<li><a class="reference external" href="http://www.iro.umontreal.ca/~lisa/pointeurs/nips2012_deep_workshop_theano_final.pdf">[pdf]</a> Bastien, Frédéric, Lamblin, Pascal, Pascanu, Razvan, Bergstra, James, Goodfellow, Ian, Bergeron, Arnaud, Bouchard, Nicolas, and Bengio, Yoshua. Theano: new features and speed improvements. NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2012.</li>
<li><a class="reference external" href="http://www.iro.umontreal.ca/~lisa/pointeurs/theano_scipy2010.pdf">[pdf]</a> Bergstra, James, Breuleux, Olivier, Bastien, Frédéric, Lamblin, Pascal, Pascanu, Razvan, Desjardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010.</li>
</ul>
<p>Thank you!</p>
</div>
<div class="section" id="contact">
<h3>Contact<a class="headerlink" href="#contact" title="Permalink to this headline">¶</a></h3>
<p>Please email to
<code class="docutils literal"><span class="pre">Grégoire</span> <span class="pre">Mesnil</span> <span class="pre">(first-add-a-dot-last-add-at-gmail-add-a-dot-com)</span></code>
for any problem report or feedback. We will be glad to hear from you.</p>
</div>
</div>
<div class="section" id="task">
<h2>Task<a class="headerlink" href="#task" title="Permalink to this headline">¶</a></h2>
<p>The Slot-Filling (Spoken Language Understanding) consists in assigning a label
to each word given a sentence. It&#8217;s a classification task.</p>
</div>
<div class="section" id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Permalink to this headline">¶</a></h2>
<p>An old and small benchmark for this task is the ATIS (Airline Travel Information
System) dataset collected by DARPA. Here is a sentence (or utterance) example using the
<a class="reference external" href="http://en.wikipedia.org/wiki/Inside_Outside_Beginning">Inside Outside Beginning (IOB)</a> representation.</p>
<table border="1" class="docutils">
<colgroup>
<col width="28%" />
<col width="8%" />
<col width="11%" />
<col width="7%" />
<col width="11%" />
<col width="4%" />
<col width="10%" />
<col width="10%" />
<col width="11%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><strong>Input</strong> (words)</td>
<td>show</td>
<td>flights</td>
<td>from</td>
<td>Boston</td>
<td>to</td>
<td>New</td>
<td>York</td>
<td>today</td>
</tr>
<tr class="row-even"><td><strong>Output</strong> (labels)</td>
<td>O</td>
<td>O</td>
<td>O</td>
<td>B-dept</td>
<td>O</td>
<td>B-arr</td>
<td>I-arr</td>
<td>B-date</td>
</tr>
</tbody>
</table>
<p>The ATIS offical split contains 4,978/893 sentences for a total of 56,590/9,198
words (average sentence length is 15) in the train/test set.  The number of
classes (different slots) is 128 including the O label (NULL).</p>
<p>As <a class="reference external" href="http://research.microsoft.com/en-us/um/people/gzweig/Pubs/Interspeech2013RNNLU.pdf">Microsoft Research people</a>,
we deal with unseen words in the test set by marking any words with only one
single occurrence in the training set as <code class="docutils literal"><span class="pre">&lt;UNK&gt;</span></code> and use this token to
represent those unseen words in the test set. As <a class="reference external" href="http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf">Ronan Collobert and colleagues</a>, we converted
sequences of numbers with the string <code class="docutils literal"><span class="pre">DIGIT</span></code> i.e. <code class="docutils literal"><span class="pre">1984</span></code> is converted to
<code class="docutils literal"><span class="pre">DIGITDIGITDIGITDIGIT</span></code>.</p>
<p>We split the official train set into a training and validation set that contain
respectively 80% and 20% of the official training sentences. <a class="reference external" href="http://research.microsoft.com/en-us/um/people/gzweig/Pubs/Interspeech2013RNNLU.pdf">Significant
performance improvement difference has to be greater than 0.6% in F1 measure at
the 95% level due to the small size of the dataset</a>.
For evaluation purpose, experiments have to report the following metrics:</p>
<ul class="simple">
<li><a class="reference external" href="http://en.wikipedia.org/wiki/Precision_(information_retrieval)">Precision</a></li>
<li><a class="reference external" href="http://en.wikipedia.org/wiki/Recall_(information_retrieval)">Recall</a></li>
<li><a class="reference external" href="http://en.wikipedia.org/wiki/F1_score">F1 score</a></li>
</ul>
<p>We will use the <a class="reference external" href="http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt">conlleval</a> PERL script to
measure the performance of our models.</p>
</div>
<div class="section" id="recurrent-neural-network-model">
<h2>Recurrent Neural Network Model<a class="headerlink" href="#recurrent-neural-network-model" title="Permalink to this headline">¶</a></h2>
<div class="section" id="raw-input-encoding">
<h3>Raw input encoding<a class="headerlink" href="#raw-input-encoding" title="Permalink to this headline">¶</a></h3>
<p>A token corresponds to a word. Each token in the ATIS vocabulary is associated to an index. Each sentence is a
array of indexes (<code class="docutils literal"><span class="pre">int32</span></code>). Then, each set (train, valid, test) is a list of arrays of indexes. A python
dictionary is defined for mapping the space of indexes to the space of words.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sentence</span>
<span class="go">array([383, 189,  13, 193, 208, 307, 195, 502, 260, 539,</span>
<span class="go">        7,  60,  72, 8, 350, 384], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">index2word</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">sentence</span><span class="p">)</span>
<span class="go">[&#39;please&#39;, &#39;find&#39;, &#39;a&#39;, &#39;flight&#39;, &#39;from&#39;, &#39;miami&#39;, &#39;florida&#39;,</span>
<span class="go">        &#39;to&#39;, &#39;las&#39;, &#39;vegas&#39;, &#39;&lt;UNK&gt;&#39;, &#39;arriving&#39;, &#39;before&#39;, &#39;DIGIT&#39;, &quot;o&#39;clock&quot;, &#39;pm&#39;]</span>
</pre></div>
</div>
<p>Same thing for labels corresponding to this particular sentence.</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span>
<span class="go">array([126, 126, 126, 126, 126,  48,  50, 126,  78, 123,  81, 126,  15,</span>
<span class="go">        14,  89,  89], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">index2label</span><span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="n">labels</span><span class="p">)</span>
<span class="go">[&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-fromloc.city_name&#39;, &#39;B-fromloc.state_name&#39;,</span>
<span class="go">        &#39;O&#39;, &#39;B-toloc.city_name&#39;, &#39;I-toloc.city_name&#39;, &#39;B-toloc.state_name&#39;,</span>
<span class="go">        &#39;O&#39;, &#39;B-arrive_time.time_relative&#39;, &#39;B-arrive_time.time&#39;,</span>
<span class="go">        &#39;I-arrive_time.time&#39;, &#39;I-arrive_time.time&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="context-window">
<h3>Context window<a class="headerlink" href="#context-window" title="Permalink to this headline">¶</a></h3>
<p>Given a sentence i.e. an array of indexes, and a window size i.e. 1,3,5,..., we
need to convert each word in the sentence to a context window surrounding this
particular word. In details, we have:</p>
<p>The index <code class="docutils literal"><span class="pre">-1</span></code> corresponds to the <code class="docutils literal"><span class="pre">PADDING</span></code> index we insert at the
beginning/end of the sentence.</p>
<p>Here is a sample:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span>
<span class="go">array([0, 1, 2, 3, 4], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">contextwin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="go">[[-1, 0, 1],</span>
<span class="go"> [ 0, 1, 2],</span>
<span class="go"> [ 1, 2, 3],</span>
<span class="go"> [ 2, 3, 4],</span>
<span class="go"> [ 3, 4,-1]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">contextwin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="go">[[-1, -1, -1, 0, 1, 2, 3],</span>
<span class="go"> [-1, -1,  0, 1, 2, 3, 4],</span>
<span class="go"> [-1,  0,  1, 2, 3, 4,-1],</span>
<span class="go"> [ 0,  1,  2, 3, 4,-1,-1],</span>
<span class="go"> [ 1,  2,  3, 4,-1,-1,-1]]</span>
</pre></div>
</div>
<p>To summarize, we started with an array of indexes and ended with a matrix of
indexes. Each line corresponds to the context window surrounding this word.</p>
</div>
<div class="section" id="word-embeddings">
<h3>Word embeddings<a class="headerlink" href="#word-embeddings" title="Permalink to this headline">¶</a></h3>
<p>Once we have the sentence converted to context windows i.e. a matrix of indexes, we have to associate
these indexes to the embeddings (real-valued vector associated to each word).
Using Theano, it gives:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">theano</span><span class="o">,</span> <span class="nn">numpy</span>
<span class="kn">from</span> <span class="nn">theano</span> <span class="k">import</span> <span class="n">tensor</span> <span class="k">as</span> <span class="n">T</span>

<span class="c1"># nv :: size of our vocabulary</span>
<span class="c1"># de :: dimension of the embedding space</span>
<span class="c1"># cs :: context window size</span>
<span class="n">nv</span><span class="p">,</span> <span class="n">de</span><span class="p">,</span> <span class="n">cs</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">5</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="mf">0.2</span> <span class="o">*</span> <span class="n">numpy</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> \
    <span class="p">(</span><span class="n">nv</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">de</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span><span class="p">))</span> <span class="c1"># add one for PADDING at the end</span>

<span class="n">idxs</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">imatrix</span><span class="p">()</span> <span class="c1"># as many columns as words in the context window and as many lines as words in the sentence</span>
<span class="n">x</span>    <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="p">[</span><span class="n">idxs</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">idxs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">de</span><span class="o">*</span><span class="n">cs</span><span class="p">))</span>
</pre></div>
</div>
<p>The x symbolic variable corresponds to a matrix of shape (number of words in the
sentences, dimension of the embedding space X context window size).</p>
<p>Let&#8217;s compile a theano function to do so</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sample</span>
<span class="go">array([0, 1, 2, 3, 4], dtype=int32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">csample</span> <span class="o">=</span> <span class="n">contextwin</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="go">[[-1, -1, -1, 0, 1, 2, 3],</span>
<span class="go"> [-1, -1,  0, 1, 2, 3, 4],</span>
<span class="go"> [-1,  0,  1, 2, 3, 4,-1],</span>
<span class="go"> [ 0,  1,  2, 3, 4,-1,-1],</span>
<span class="go"> [ 1,  2,  3, 4,-1,-1,-1]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">idxs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="p">(</span><span class="n">csample</span><span class="p">)</span>
<span class="go">array([[-0.08088442,  0.08458307,  0.05064092, ...,  0.06876887,</span>
<span class="go">        -0.06648078, -0.15192257],</span>
<span class="go">       [-0.08088442,  0.08458307,  0.05064092, ...,  0.11192625,</span>
<span class="go">         0.08745284,  0.04381778],</span>
<span class="go">       [-0.08088442,  0.08458307,  0.05064092, ..., -0.00937143,</span>
<span class="go">         0.10804889,  0.1247109 ],</span>
<span class="go">       [ 0.11038255, -0.10563177, -0.18760249, ..., -0.00937143,</span>
<span class="go">         0.10804889,  0.1247109 ],</span>
<span class="go">       [ 0.18738101,  0.14727569, -0.069544  , ..., -0.00937143,</span>
<span class="go">         0.10804889,  0.1247109 ]], dtype=float32)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f</span><span class="p">(</span><span class="n">csample</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(5, 350)</span>
</pre></div>
</div>
<p>We now have a sequence (of length 5 which is corresponds to the length of the
sentence) of <strong>context window word embeddings</strong> which is easy to feed to a simple
recurrent neural network to iterate with.</p>
</div>
<div class="section" id="elman-recurrent-neural-network">
<h3>Elman recurrent neural network<a class="headerlink" href="#elman-recurrent-neural-network" title="Permalink to this headline">¶</a></h3>
<p>The followin (Elman) recurrent neural network (E-RNN) takes as input the current input
(time <code class="docutils literal"><span class="pre">t</span></code>) and the previous hiddent state (time <code class="docutils literal"><span class="pre">t-1</span></code>). Then it iterates.</p>
<p>In the previous section, we processed the input to fit this
sequential/temporal structure.  It consists in a matrix where the row <code class="docutils literal"><span class="pre">0</span></code> corresponds to
the time step <code class="docutils literal"><span class="pre">t=0</span></code>, the row <code class="docutils literal"><span class="pre">1</span></code> corresponds to the time step  <code class="docutils literal"><span class="pre">t=1</span></code>, etc.</p>
<p>The <strong>parameters</strong> of the E-RNN to be learned are:</p>
<ul class="simple">
<li>the word embeddings (real-valued matrix)</li>
<li>the initial hidden state (real-value vector)</li>
<li>two matrices for the linear projection of the input <code class="docutils literal"><span class="pre">t</span></code> and the previous hidden layer state <code class="docutils literal"><span class="pre">t-1</span></code></li>
<li>(optional) bias. <a class="reference external" href="http://en.wikipedia.org/wiki/Occam's_razor">Recommendation</a>: don&#8217;t use it.</li>
<li>softmax classification layer on top</li>
</ul>
<p>The <strong>hyperparameters</strong> define the whole architecture:</p>
<ul class="simple">
<li>dimension of the word embedding</li>
<li>size of the vocabulary</li>
<li>number of hidden units</li>
<li>number of classes</li>
<li>random seed + way to initialize the model</li>
</ul>
<p>It gives the following code:</p>
<p>Then we integrate the way to build the input from the embedding matrix:</p>
<p>We use the scan operator to construct the recursion, works like a charm:</p>
<p>Theano will then compute all the gradients automatically to maximize the log-likelihood:</p>
<p>Next compile those functions:</p>
<p>We keep the word embeddings on the unit sphere by normalizing them after each update:</p>
<p>And that&#8217;s it!</p>
</div>
</div>
<div class="section" id="evaluation">
<h2>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h2>
<p>With the previous defined functions, you can compare the predicted labels with
the true labels and compute some metrics. In this <a class="reference external" href="https://github.com/mesnilgr/is13">repo</a>, we build a wrapper around the <a class="reference external" href="http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt">conlleval</a> PERL script.
It&#8217;s not trivial to compute those metrics due to the <a class="reference external" href="http://en.wikipedia.org/wiki/Inside_Outside_Beginning">Inside Outside Beginning
(IOB)</a> representation
i.e. a prediction is considered correct if the word-beginning <strong>and</strong> the
word-inside <strong>and</strong> the word-outside predictions are <strong>all</strong> correct.
Note that the extension is <img class="math" src="_images/math/7e04c87ca8f88d1b1b9fbb48ec8be7bdc183ba0e.png" alt="txt"/> and you will have to change it to <img class="math" src="_images/math/756f7c119ffdbd7d2cecae8c3586c38c48f14ea5.png" alt="pl"/>.</p>
</div>
<div class="section" id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h2>
<div class="section" id="updates">
<h3>Updates<a class="headerlink" href="#updates" title="Permalink to this headline">¶</a></h3>
<p>For stochastic gradient descent (SGD) update, we consider the whole sentence as a mini-batch
and perform one update per sentence. It is possible to perform a pure SGD (contrary to mini-batch)
where the update is done on only one single word at a time.</p>
<p>After each iteration/update, we normalize the word embeddings to keep them on a unit sphere.</p>
</div>
<div class="section" id="stopping-criterion">
<h3>Stopping Criterion<a class="headerlink" href="#stopping-criterion" title="Permalink to this headline">¶</a></h3>
<p>Early-stopping on a validation set is our regularization technique:
the training is run for a given number of epochs (a single pass through the
whole dataset) and keep the best model along with respect to the F1 score
computed on the validation set after each epoch.</p>
</div>
<div class="section" id="hyper-parameter-selection">
<h3>Hyper-Parameter Selection<a class="headerlink" href="#hyper-parameter-selection" title="Permalink to this headline">¶</a></h3>
<p>Although there is interesting research/<a class="reference external" href="https://github.com/JasperSnoek/spearmint">code</a> on the topic of automatic
hyper-parameter selection, we use the <a class="reference external" href="http://en.wikipedia.org/wiki/KISS_principle">KISS</a> random search.</p>
<p>The following intervals can give you some starting point:</p>
<ul class="simple">
<li>learning rate : uniform([0.05,0.01])</li>
<li>window size : random value from {3,...,19}</li>
<li>number of hidden units : random value from {100,200}</li>
<li>embedding dimension : random value from {50,100}</li>
</ul>
</div>
</div>
<div class="section" id="running-the-code">
<h2>Running the Code<a class="headerlink" href="#running-the-code" title="Permalink to this headline">¶</a></h2>
<p>After downloading the data using <img class="math" src="_images/math/f74951cc060cb8dfd9feb86ccef4be8b1bd33112.png" alt="download.sh"/>, the user can then run the code by calling:</p>
<div class="highlight-bash"><div class="highlight"><pre><span></span>python code/rnnslu.py

<span class="o">(</span><span class="s1">&#39;NEW BEST: epoch&#39;</span>, 25, <span class="s1">&#39;valid F1&#39;</span>, 96.84, <span class="s1">&#39;best test F1&#39;</span>, 93.79<span class="o">)</span>
<span class="o">[</span>learning<span class="o">]</span> epoch <span class="m">26</span> &gt;&gt; 100.00% completed in 28.76 <span class="o">(</span>sec<span class="o">)</span> &lt;&lt;
<span class="o">[</span>learning<span class="o">]</span> epoch <span class="m">27</span> &gt;&gt; 100.00% completed in 28.76 <span class="o">(</span>sec<span class="o">)</span> &lt;&lt;
...
<span class="o">(</span><span class="s1">&#39;BEST RESULT: epoch&#39;</span>, 57, <span class="s1">&#39;valid F1&#39;</span>, 97.23, <span class="s1">&#39;best test F1&#39;</span>, 94.2, <span class="s1">&#39;with the model&#39;</span>, <span class="s1">&#39;rnnslu&#39;</span><span class="o">)</span>
</pre></div>
</div>
<div class="section" id="timing">
<h3>Timing<a class="headerlink" href="#timing" title="Permalink to this headline">¶</a></h3>
<p>Running experiments on ATIS using this <a class="reference external" href="https://github.com/mesnilgr/is13">repository</a>
will run one epoch in less than 40 seconds on i7 CPU 950 &#64; 3.07GHz using less than 200 Mo of RAM:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">learning</span><span class="p">]</span> <span class="n">epoch</span> <span class="mi">0</span> <span class="o">&gt;&gt;</span> <span class="mf">100.00</span><span class="o">%</span> <span class="n">completed</span> <span class="ow">in</span> <span class="mf">34.48</span> <span class="p">(</span><span class="n">sec</span><span class="p">)</span> <span class="o">&lt;&lt;</span>
</pre></div>
</div>
<p>After a few epochs, you obtain decent performance <strong>94.48 % of F1 score</strong>.:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">NEW</span> <span class="n">BEST</span><span class="p">:</span> <span class="n">epoch</span> <span class="mi">28</span> <span class="n">valid</span> <span class="n">F1</span> <span class="mf">96.61</span> <span class="n">best</span> <span class="n">test</span> <span class="n">F1</span> <span class="mf">94.19</span>
<span class="n">NEW</span> <span class="n">BEST</span><span class="p">:</span> <span class="n">epoch</span> <span class="mi">29</span> <span class="n">valid</span> <span class="n">F1</span> <span class="mf">96.63</span> <span class="n">best</span> <span class="n">test</span> <span class="n">F1</span> <span class="mf">94.42</span>
<span class="p">[</span><span class="n">learning</span><span class="p">]</span> <span class="n">epoch</span> <span class="mi">30</span> <span class="o">&gt;&gt;</span> <span class="mf">100.00</span><span class="o">%</span> <span class="n">completed</span> <span class="ow">in</span> <span class="mf">35.04</span> <span class="p">(</span><span class="n">sec</span><span class="p">)</span> <span class="o">&lt;&lt;</span>
<span class="p">[</span><span class="n">learning</span><span class="p">]</span> <span class="n">epoch</span> <span class="mi">31</span> <span class="o">&gt;&gt;</span> <span class="mf">100.00</span><span class="o">%</span> <span class="n">completed</span> <span class="ow">in</span> <span class="mf">34.80</span> <span class="p">(</span><span class="n">sec</span><span class="p">)</span> <span class="o">&lt;&lt;</span>
<span class="p">[</span><span class="o">...</span><span class="p">]</span>
<span class="n">NEW</span> <span class="n">BEST</span><span class="p">:</span> <span class="n">epoch</span> <span class="mi">40</span> <span class="n">valid</span> <span class="n">F1</span> <span class="mf">97.25</span> <span class="n">best</span> <span class="n">test</span> <span class="n">F1</span> <span class="mf">94.34</span>
<span class="p">[</span><span class="n">learning</span><span class="p">]</span> <span class="n">epoch</span> <span class="mi">41</span> <span class="o">&gt;&gt;</span> <span class="mf">100.00</span><span class="o">%</span> <span class="n">completed</span> <span class="ow">in</span> <span class="mf">35.18</span> <span class="p">(</span><span class="n">sec</span><span class="p">)</span> <span class="o">&lt;&lt;</span>
<span class="n">NEW</span> <span class="n">BEST</span><span class="p">:</span> <span class="n">epoch</span> <span class="mi">42</span> <span class="n">valid</span> <span class="n">F1</span> <span class="mf">97.33</span> <span class="n">best</span> <span class="n">test</span> <span class="n">F1</span> <span class="mf">94.48</span>
<span class="p">[</span><span class="n">learning</span><span class="p">]</span> <span class="n">epoch</span> <span class="mi">43</span> <span class="o">&gt;&gt;</span> <span class="mf">100.00</span><span class="o">%</span> <span class="n">completed</span> <span class="ow">in</span> <span class="mf">35.39</span> <span class="p">(</span><span class="n">sec</span><span class="p">)</span> <span class="o">&lt;&lt;</span>
<span class="p">[</span><span class="n">learning</span><span class="p">]</span> <span class="n">epoch</span> <span class="mi">44</span> <span class="o">&gt;&gt;</span> <span class="mf">100.00</span><span class="o">%</span> <span class="n">completed</span> <span class="ow">in</span> <span class="mf">35.31</span> <span class="p">(</span><span class="n">sec</span><span class="p">)</span> <span class="o">&lt;&lt;</span>
<span class="p">[</span><span class="o">...</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="word-embedding-nearest-neighbors">
<h3>Word Embedding Nearest Neighbors<a class="headerlink" href="#word-embedding-nearest-neighbors" title="Permalink to this headline">¶</a></h3>
<p>We can check the k-nearest neighbors of the learned embeddings. L2 and
cosine distance gave the same results so we plot them for the cosine distance.</p>
<table border="1" class="docutils">
<colgroup>
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
<col width="10%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>atlanta</strong></th>
<th class="head"><strong>back</strong></th>
<th class="head"><strong>ap80</strong></th>
<th class="head"><strong>but</strong></th>
<th class="head"><strong>aircraft</strong></th>
<th class="head"><strong>business</strong></th>
<th class="head"><strong>a</strong></th>
<th class="head"><strong>august</strong></th>
<th class="head"><strong>actually</strong></th>
<th class="head"><strong>cheap</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>phoenix</td>
<td>live</td>
<td>ap57</td>
<td>if</td>
<td>plane</td>
<td>coach</td>
<td>people</td>
<td>september</td>
<td>provide</td>
<td>weekday</td>
</tr>
<tr class="row-odd"><td>denver</td>
<td>lives</td>
<td>ap</td>
<td>up</td>
<td>service</td>
<td>first</td>
<td>do</td>
<td>january</td>
<td>prices</td>
<td>weekdays</td>
</tr>
<tr class="row-even"><td>tacoma</td>
<td>both</td>
<td>connections</td>
<td>a</td>
<td>airplane</td>
<td>fourth</td>
<td>but</td>
<td>june</td>
<td>stop</td>
<td>am</td>
</tr>
<tr class="row-odd"><td>columbus</td>
<td>how</td>
<td>tomorrow</td>
<td>now</td>
<td>seating</td>
<td>thrift</td>
<td>numbers</td>
<td>december</td>
<td>number</td>
<td>early</td>
</tr>
<tr class="row-even"><td>seattle</td>
<td>me</td>
<td>before</td>
<td>amount</td>
<td>stand</td>
<td>tenth</td>
<td>abbreviation</td>
<td>november</td>
<td>flight</td>
<td>sfo</td>
</tr>
<tr class="row-odd"><td>minneapolis</td>
<td>out</td>
<td>earliest</td>
<td>more</td>
<td>that</td>
<td>second</td>
<td>if</td>
<td>april</td>
<td>there</td>
<td>milwaukee</td>
</tr>
<tr class="row-even"><td>pittsburgh</td>
<td>other</td>
<td>connect</td>
<td>abbreviation</td>
<td>on</td>
<td>fifth</td>
<td>up</td>
<td>july</td>
<td>serving</td>
<td>jfk</td>
</tr>
<tr class="row-odd"><td>ontario</td>
<td>plane</td>
<td>thrift</td>
<td>restrictions</td>
<td>turboprop</td>
<td>third</td>
<td>serve</td>
<td>jfk</td>
<td>thank</td>
<td>shortest</td>
</tr>
<tr class="row-even"><td>montreal</td>
<td>service</td>
<td>coach</td>
<td>mean</td>
<td>mean</td>
<td>twelfth</td>
<td>database</td>
<td>october</td>
<td>ticket</td>
<td>bwi</td>
</tr>
<tr class="row-odd"><td>philadelphia</td>
<td>fare</td>
<td>today</td>
<td>interested</td>
<td>amount</td>
<td>sixth</td>
<td>passengers</td>
<td>may</td>
<td>are</td>
<td>lastest</td>
</tr>
</tbody>
</table>
<p>As you can judge, the limited size of the vocabulary (about 500 words) gives us mitigated
performance. According to human judgement: some are good, some are bad.</p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="lstm.html" title="LSTM Networks for Sentiment Analysis"
             >next</a> |</li>
        <li class="right" >
          <a href="hmc.html" title="Hybrid Monte-Carlo Sampling"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="contents.html">DeepLearning 0.1 documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2008--2010, LISA lab.
      Last updated on Sep 25, 2017.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.5.1.
    </div>
  </body>
</html>